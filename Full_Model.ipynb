{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51d2898",
   "metadata": {},
   "source": [
    "**DESCRIPTION:** This model will run a regression on all of the data over the time period given, treating identifier, market cap, the factors, etc as independent variables. \n",
    "The regression is completed using SKLearn which utilizes utilizes test and training data to fit a learned model to the data. The model will then be used to complete a factor selection and forward/backward factor selection which can be used to fine-tune the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c94e1",
   "metadata": {},
   "source": [
    "**How to Run This Code:** Run each segment of code and markdown in order using the 'Run' button above. Once you have run the last segment the code will execute and results will be outputted. It may take a few moments for the code to output results so please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7532985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from time import time\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f197e",
   "metadata": {},
   "source": [
    "**Calculating the Model:** The function below calculates and plots our model given a datatable. Comments included in the code provide a description of what is occuring in each step. **NOTE:** The function for forward and backward selection has been commented out, as it takes a significant amount of time due to the size of the dataset. After running the code once, you may uncomment the function and run it again to see the results including the forward + backward regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model(data, predictor_list):\n",
    "    # Sets the index of the graph as the date so that the regression occurs\n",
    "    # over the dates\n",
    "    df.set_index(pd.DatetimeIndex(df['date']), inplace=True)\n",
    "\n",
    "    # Sets the predictor values - i.e. the independent variables that will be used in the regression\n",
    "    predictors = predictor_list\n",
    "\n",
    "    # Uses the train_test_split to randomly select 30% of the data as testing\n",
    "    # data and saving the rest for the creation/training of the model\n",
    "    train, test = train_test_split(data, test_size=0.3)\n",
    "\n",
    "    # Defines the model as a Linear Regression\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Fits the model using the predictors above and defined target training data\n",
    "    model.fit(train[predictors], train[\"target\"])\n",
    "\n",
    "    # Creates the predictors using the model.predict\n",
    "    preds = model.predict(test[predictors])\n",
    "    preds = pd.Series(preds, index=test.index)\n",
    "    \n",
    "    # Calculates the r^2 score based on the test values and the predicted values\n",
    "    r = r2_score(test[\"target\"], preds)\n",
    "\n",
    "    # Creates a graph that plots the predictions of the model against true values\n",
    "    combined = pd.concat({\"target\": test[\"target\"], \"Predictions\": preds},\n",
    "                         axis=1)\n",
    "\n",
    "    # Plots the origional vs predicted\n",
    "    combined.plot()\n",
    "    plt.title(\"Full Model\")\n",
    "    plt.show()\n",
    "    \n",
    "    # conducts a k_fold test on the data using\n",
    "    # predictors (x) and response variables (y)\n",
    "    y = data['target']\n",
    "    X = data[predictor_list]\n",
    "\n",
    "    # Conducts the K_Fold test\n",
    "    cv = KFold(n_splits=10, random_state=100, shuffle=True)\n",
    "\n",
    "    # Calculates the cross validation score which is later used to calculate the\n",
    "    # Root Mean Squared Error below. The RMSE is useful as it provides information regarding the accuracy of our model\n",
    "    # the lower the RMSE the better\n",
    "    scores = cross_val_score(model, train[predictors], train[\"target\"],\n",
    "                             scoring='neg_mean_absolute_error',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "    \n",
    "    print(\"r^2 is = \" + str(r))\n",
    "    print(\"root mean squared error (RMSE) = \" + str(np.sqrt(np.mean(np.absolute(\n",
    "        scores)))))\n",
    "\n",
    "    #Setting the features that will be used throughou the rest of the code\n",
    "    features = ['sector','market_cap', 'index_membership', 'factor_1',\n",
    "                'factor_2', 'factor_3', 'factor_4', 'factor_5', 'factor_6',\n",
    "                'factor_7', 'factor_8', 'factor_9', 'factor_10']\n",
    "        \n",
    "      \n",
    "    # Uses RidgeCV to compute efficient Leave-One-Out Cross-Validation. This is an itteration of the Kfold where \n",
    "    # the number of folds equals the number of instances in the data set.\n",
    "    # This is advantageous as it is usefull in models that may have multicollinearity.\n",
    "     \n",
    "    ridge = RidgeCV(alphas = np.logspace(-6, 6, num=5)).fit(X, y)\n",
    "        \n",
    "    # Visualizing Feature Importance by a factor of 1000 for easier visibility\n",
    "    # Also selects best features based on the ridge scores and analysis\n",
    "        \n",
    "    feature_importance(X, y, ridge)\n",
    "        \n",
    "    # Selects the most relevent features using forward and backward\n",
    "    # Sequential Feature Selection\n",
    "        \n",
    "    # TO:DO comment out this once you have run the function once\n",
    "    \n",
    "    # seq_selection(X, y, features, ridge)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c64c5",
   "metadata": {},
   "source": [
    "**Feature Importance of Coefficients using RidgeCV:** These functions will utilize the RidgeCV computed in the above function to determine the most important fratures based on a relevency threshhold. It also plots the feature importance graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple helper function to go from list of True and False to the final selected values\n",
    "\n",
    "def selecting(lst, features):\n",
    "    selected = []\n",
    "    for i in range(0,13):\n",
    "        if lst[i] == True:\n",
    "            selected.append(features[i])\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c3186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(X, y, ridge):\n",
    "        \n",
    "    # sets the importance as a list of the ridge coefficients calculated for each feature\n",
    "    importance = list(np.abs(ridge.coef_))\n",
    "\n",
    "    # re-itterates the features, but using better spacing for graph visibility\n",
    "    features = ['sector','market \\n cap', 'index \\n membership', 'factor_1',\n",
    "            'factor_2', 'factor_3', 'factor_4', 'factor_5', 'factor_6',\n",
    "            'factor_7', 'factor_8', 'factor_9', 'factor_10']\n",
    "\n",
    "    # preps a dictionary which scales each 'importance score' by 1000 for visbility\n",
    "    dta = {}\n",
    "    importance_2 = []\n",
    "    \n",
    "    for i in range(0,13):\n",
    "        importance_2.append(importance[i] *1000)\n",
    "        dta[features[i]] = importance_2[i]\n",
    "    \n",
    "    # graphs the factors vs importance level\n",
    "    names = list(dta.keys())\n",
    "    values = list(dta.values())\n",
    "\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(16, 3), sharey=True)\n",
    "    plt.bar(names, values)\n",
    "    fig.suptitle('Feature importances via coefficients (scaled by 1000)')\n",
    "\n",
    "    # Selecting features based on importance\n",
    "    \n",
    "    # sets a threshold parameter and will select the features whose importance (defined by the coefficients) are above this threshold\n",
    "    # This specific threshold will select 3 features by setting the threshold slightly above the coefficient of the fourth most important feature\n",
    "    threshold = np.sort(importance)[-4] + 0.01\n",
    "    # time is used to show the user how long the calculation took - start\n",
    "    tic = time()\n",
    "    # selects the features from the model that are above the threshhold\n",
    "    sfm = SelectFromModel(ridge, threshold=threshold).fit(X, y)\n",
    "    # end time calculation\n",
    "    toc = time()\n",
    "    # returns the list of features found in sfm, i.e. those above the threshhold\n",
    "    lst = list(sfm.get_support())\n",
    "    # prints the results\n",
    "    print(f\"Features selected by SelectFromModel:\", selecting(lst, features))\n",
    "    print(f\"Done in {toc - tic:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f287254",
   "metadata": {},
   "source": [
    "**Selecting features with Sequential Feature Selection:** SFS is a greedy procedure where, at each iteration, we choose the best new feature to add to our selected features based a cross-validation score (starts with 0 features and choose the best single feature with the highest score. The procedure is repeated until we reach the desired number of selected features) Our function below does this in forward and backward SFS (the reverse of what is described above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd7782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_selection(X, y, features, ridge):\n",
    "\n",
    "    # Forward Selection\n",
    "    \n",
    "    # starts the time\n",
    "    tic_fwd = time()\n",
    "    # runs the SequentialFeatureSelector on our ridge values previosuly calculated\n",
    "    # and specifies that this is to be done in the forward direction\n",
    "    sfs_forward = SequentialFeatureSelector(\n",
    "        ridge, n_features_to_select=2, direction=\"forward\"\n",
    "    ).fit(X, y)\n",
    "    # stops the time\n",
    "    toc_fwd = time()\n",
    "    \n",
    "    # Backward Selection\n",
    "    \n",
    "    # starts the time\n",
    "    tic_bwd = time()\n",
    "    # runs the SequentialFeatureSelector on our ridge values previosuly calculated\n",
    "    # and specifies that this is to be done in the backward direction\n",
    "    sfs_backward = SequentialFeatureSelector(\n",
    "        ridge, n_features_to_select=2, direction=\"backward\"\n",
    "    ).fit(X, y)\n",
    "    # stops the time\n",
    "    toc_bwd = time()\n",
    "\n",
    "    #prints the results\n",
    "    print(\n",
    "        \"Features selected by forward sequential selection: \", selecting(sfs_forward.get_support(), features)\n",
    "    )\n",
    "    print(f\"Done in {toc_fwd - tic_fwd:.3f}s\")\n",
    "    print(\n",
    "        \"Features selected by backward sequential selection: \", selecting(sfs_backward.get_support(), features)\n",
    "    )\n",
    "    print(f\"Done in {toc_bwd - tic_bwd:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f421a0",
   "metadata": {},
   "source": [
    "**Note on Path to File:** Below I have specified the path to the given CSV containing the data. You do not need to change the path for it to work on this notebook, but if you would like to download the code, you may."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"data/data.csv\"\n",
    "\n",
    "# Reads the information contained in the CSV\n",
    "df = pd.read_csv(path_to_file)\n",
    "\n",
    "# Turns the identifiers into dummy variables for the regression\n",
    "df = pd.get_dummies(df,prefix='Identifier ', prefix_sep='=', columns=[\n",
    "    'identifier'])\n",
    "\n",
    "# Setting the list predictor variables called cols\n",
    "cols = list(df.columns)\n",
    "\n",
    "cols.remove('target')\n",
    "cols.remove('date')\n",
    "\n",
    "# Running the regression with the given dataset and predictor list\n",
    "calculate_model(df, cols)\n",
    "\n",
    "# signals that the code has run to completion\n",
    "print('\\n Model Complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
