{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51d2898",
   "metadata": {},
   "source": [
    "**DESCRIPTION:** This model will run a regression on all of the data over the time period given, treating identifier, market cap, the factors, etc as independent variables. \n",
    "The regression is completed using SKLearn which utilizes utilizes test and training data to fit a learned model to the data. The model will then be used to complete a factor selection and forward/backward factor selection which can be used to fine-tune the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c94e1",
   "metadata": {},
   "source": [
    "**How to Run This Code:** Run each segment of code and markdown in order using the 'Run' button above. Once you have run the last segment the code will execute and results will be outputted. It may take a few moments for the code to output results so please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7532985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from time import time\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df3094",
   "metadata": {},
   "source": [
    "**Arranging the Data:** This function will arrange our data based on the given dataset to turn the dates into the index of the graph to allow for the regression to occur over the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6c2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_data(data):\n",
    "\n",
    "    data.set_index(pd.DatetimeIndex(df['date']), inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a1070a",
   "metadata": {},
   "source": [
    "**Getting the Training and Testing Data:** This code sets the predictor values, i.e. the independent variables that will be used in the regression and uses the train_test_split to randomly select 30% of the data as testing data, saving the rest for the creation/training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test_data(data, predictor_list):\n",
    "\n",
    "    predictors = predictor_list\n",
    "\n",
    "    train, test = train_test_split(data, test_size=0.3)\n",
    "    \n",
    "    return [train, test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8241aae1",
   "metadata": {},
   "source": [
    "**Running the Linear Regression:** This function conducts the linear regression by fitting thr model to the predictors and training data. It then creats predictions for the data using model.predict and using the index we set previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0bac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(train, test, predictors):\n",
    "    \n",
    "    # Defines the model as a Linear Regression\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Fits the model using the predictors above and defined target training data\n",
    "    model.fit(train[predictors], train[\"target\"])\n",
    "\n",
    "    # Creates the predictors using the model.predict\n",
    "    preds = model.predict(test[predictors])\n",
    "    preds = pd.Series(preds, index=test.index)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69af88f2",
   "metadata": {},
   "source": [
    "**Calculating r^2:** This function calculates the r^2 of the model by utilizing the r2_score function with the test target data and predictions from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c931d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(preds, test):\n",
    "    \n",
    "    # Calculates the r^2 score based on the test values and the predicted values\n",
    "    r = r2_score(test[\"target\"], preds)\n",
    "    \n",
    "    print(\"r^2 is = \" + str(r) + \"\\n\")\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f669d",
   "metadata": {},
   "source": [
    "**Graphing the Results:** This function creates a graph that plots the predictions of the model against the true values of the test target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149bc104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_preds_vs_test(preds, test):\n",
    "    # Creates a graph that plots the predictions of the model against true values\n",
    "    combined = pd.concat({\"target\": test[\"target\"], \"Predictions\": preds},\n",
    "                         axis=1)\n",
    "\n",
    "    # Plots the origional vs predicted\n",
    "    combined.plot()\n",
    "    plt.title(\"Full Model\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a90ddf0",
   "metadata": {},
   "source": [
    "**K Fold Test:** Conducts a k_fold test on the data using the predictors (X) and response variables (y) using the KFold function. It then Calculates the cross validation score which is later used to calculate the Root Mean Squared Error. The RMSE is useful as it provides information regarding the accuracy of our model - i.e. the lower the RMSE the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2980bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_test(predictor_list, train, model, X, y):\n",
    "\n",
    "    # Conducts the K_Fold test\n",
    "    cv = KFold(n_splits=10, random_state=100, shuffle=True)\n",
    "\n",
    "    scores = cross_val_score(model, train[predictor_list], train[\"target\"],\n",
    "                             scoring='neg_mean_absolute_error',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "    \n",
    "    print(\"root mean squared error (RMSE) = \" + str(np.sqrt(np.mean(np.absolute(\n",
    "        scores)))) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79116e0",
   "metadata": {},
   "source": [
    "**Features:** Returns the features to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e047317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_features():\n",
    "    return ['sector','market_cap', 'index_membership', 'factor_1',\n",
    "                'factor_2', 'factor_3', 'factor_4', 'factor_5', 'factor_6',\n",
    "                'factor_7', 'factor_8', 'factor_9', 'factor_10']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5b8c6",
   "metadata": {},
   "source": [
    "**Set X:** Defines our X, i.e. predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c2ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_X(data, predictor_list):\n",
    "    return data[predictor_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dbf73d",
   "metadata": {},
   "source": [
    "**Set Y:** Defines our y, i.e.  response variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e608f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_y(data):\n",
    "    return data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2e8dc",
   "metadata": {},
   "source": [
    "**Ridge Calculation:** Uses RidgeCV to compute efficient Leave-One-Out Cross-Validation. This is an itteration of the Kfold where the number of folds equals the number of instances in the data set. This is advantageous as it is usefull in models that may have multicollinearity.\n",
    "* Used in feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790da69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_calculation(X, y):\n",
    "    \n",
    "    ridge = RidgeCV(alphas = np.logspace(-6, 6, num=5)).fit(X, y)\n",
    "    \n",
    "    return ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c64c5",
   "metadata": {},
   "source": [
    "**Feature Importance of Coefficients using RidgeCV:** These functions will utilize the RidgeCV computed in the above function to determine the most important fratures based on a relevency threshhold. It also plots the feature importance graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple helper function to go from list of True and False to the final selected values\n",
    "\n",
    "def selecting(lst, features):\n",
    "    selected = []\n",
    "    for i in range(0,13):\n",
    "        if lst[i] == True:\n",
    "            selected.append(features[i])\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c3186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(X, y, ridge):\n",
    "        \n",
    "    # sets the importance as a list of the ridge coefficients calculated for each feature\n",
    "    importance = list(np.abs(ridge.coef_))\n",
    "\n",
    "    # re-itterates the features, but using better spacing for graph visibility\n",
    "    features = ['sector','market \\n cap', 'index \\n membership', 'factor_1',\n",
    "            'factor_2', 'factor_3', 'factor_4', 'factor_5', 'factor_6',\n",
    "            'factor_7', 'factor_8', 'factor_9', 'factor_10']\n",
    "\n",
    "    # preps a dictionary which scales each 'importance score' by 1000 for visbility\n",
    "    dta = {}\n",
    "    importance_2 = []\n",
    "    \n",
    "    for i in range(0,13):\n",
    "        importance_2.append(importance[i] *1000)\n",
    "        dta[features[i]] = importance_2[i]\n",
    "    \n",
    "    # graphs the factors vs importance level\n",
    "    names = list(dta.keys())\n",
    "    values = list(dta.values())\n",
    "\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(16, 3), sharey=True)\n",
    "    plt.bar(names, values)\n",
    "    fig.suptitle('Feature importances via coefficients (scaled by 1000)')\n",
    "\n",
    "    # Selecting features based on importance\n",
    "    \n",
    "    # sets a threshold parameter and will select the features whose importance (defined by the coefficients) are above this threshold\n",
    "    # This specific threshold will select 3 features by setting the threshold slightly above the coefficient of the fourth most important feature\n",
    "    threshold = np.sort(importance)[-4] + 0.01\n",
    "    # time is used to show the user how long the calculation took - start\n",
    "    tic = time()\n",
    "    # selects the features from the model that are above the threshhold\n",
    "    sfm = SelectFromModel(ridge, threshold=threshold).fit(X, y)\n",
    "    # end time calculation\n",
    "    toc = time()\n",
    "    # returns the list of features found in sfm, i.e. those above the threshhold\n",
    "    lst = list(sfm.get_support())\n",
    "    # prints the results\n",
    "    print(f\"Features selected by SelectFromModel:\", selecting(lst, features))\n",
    "    print(f\"Done in {toc - tic:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f287254",
   "metadata": {},
   "source": [
    "**Selecting features with Sequential Feature Selection:** SFS is a greedy procedure where, at each iteration, we choose the best new feature to add to our selected features based a cross-validation score (starts with 0 features and choose the best single feature with the highest score. The procedure is repeated until we reach the desired number of selected features) Our function below does this in forward and backward SFS (the reverse of what is described above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd7782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_selection(X, y, features, ridge):\n",
    "\n",
    "    # Forward Selection\n",
    "    \n",
    "    # starts the time\n",
    "    tic_fwd = time()\n",
    "    # runs the SequentialFeatureSelector on our ridge values previosuly calculated\n",
    "    # and specifies that this is to be done in the forward direction\n",
    "    sfs_forward = SequentialFeatureSelector(\n",
    "        ridge, scoring='r2', direction=\"forward\"\n",
    "    ).fit(X, y)\n",
    "    # stops the time\n",
    "    toc_fwd = time()\n",
    "    \n",
    "    # Backward Selection\n",
    "    \n",
    "    # starts the time\n",
    "    tic_bwd = time()\n",
    "    # runs the SequentialFeatureSelector on our ridge values previosuly calculated\n",
    "    # and specifies that this is to be done in the backward direction\n",
    "    sfs_backward = SequentialFeatureSelector(\n",
    "        ridge, scoring='r2', direction=\"backward\"\n",
    "    ).fit(X, y)\n",
    "    # stops the time\n",
    "    toc_bwd = time()\n",
    "\n",
    "    #prints the results\n",
    "    print(\n",
    "        \"Features selected by forward sequential selection: \", selecting(sfs_forward.get_support(), features)\n",
    "    )\n",
    "    print(f\"Done in {toc_fwd - tic_fwd:.3f}s\")\n",
    "    print(\n",
    "        \"Features selected by backward sequential selection: \", selecting(sfs_backward.get_support(), features)\n",
    "    )\n",
    "    print(f\"Done in {toc_bwd - tic_bwd:.3f}s\")\n",
    "    \n",
    "    return selecting(sfs_backward.get_support(), features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a299e58",
   "metadata": {},
   "source": [
    "**Calculating the Model:** The function below calculates and plots our model given a datatable. Comments included in the code provide a description of what is occuring in each step. **NOTE:** The function for forward and backward selection has been commented out, as it takes a significant amount of time due to the size of the dataset. After running the code once, you may uncomment the function and run it again to see the results including the forward + backward regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, predictor_list):\n",
    "    \n",
    "    print('1) Arranging the dataset to factor the dates as the index and specify for the chosen identifier \\n')\n",
    "        \n",
    "    ds = arrange_data(data);\n",
    "    \n",
    "    print('2) Running the linear regression \\n')\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    \n",
    "    print('3) Partitioning the training and testing data \\n')\n",
    "    \n",
    "    train_data = get_train_and_test_data(ds, predictor_list)[0]\n",
    "    \n",
    "    test_data = get_train_and_test_data(ds, predictor_list)[1]\n",
    "    \n",
    "    print('4) Making predictions \\n')\n",
    "    \n",
    "    predictions = linear_model(train_data, test_data, predictor_list)\n",
    "    \n",
    "    print('5) Calculating the r^2 for the full model \\n')\n",
    "    \n",
    "    r_2 = r2(predictions, test_data)\n",
    "    \n",
    "    print('6) Graphing the model of test vs predicted \\n')\n",
    "    \n",
    "    graph_preds_vs_test(predictions, test_data)\n",
    "    \n",
    "    X = set_X(ds, predictor_list)\n",
    "    \n",
    "    y = set_y(ds)\n",
    "    \n",
    "    print('6) Running the K Fold test and Ridge Calculation \\n')\n",
    "    \n",
    "    k_fold_test(predictor_list, train_data, model, X, y)\n",
    "    \n",
    "    ridge = ridge_calculation(X, y)\n",
    "    \n",
    "    print('7) Graphing feature importance \\n')\n",
    "    \n",
    "    feature_importance(X, y, ridge)\n",
    "    \n",
    "    # Selects the most relevent features using forward and backward\n",
    "    # Sequential Feature Selection\n",
    "        \n",
    "    features = set_features()\n",
    "        \n",
    "    # TO:DO comment all of the code below out once you have run the function once\n",
    "    \n",
    "    # print('8) Optimizing feature selection using forward and backward selection \\n')\n",
    "    \n",
    "    # new_predictors = list(seq_selection(X, y, features, ridge, iden))\n",
    "    \n",
    "    # print('\\n 9) Creating the model with the selected identifiers in Forward Training \\n')\n",
    "        \n",
    "    # train_data2 = get_train_and_test_data(ds, new_predictors)[0]\n",
    "        \n",
    "    # test_data2 = get_train_and_test_data(ds, new_predictors)[1]\n",
    "        \n",
    "    # predictions2 = linear_model(train_data2, test_data2, new_predictors)\n",
    "        \n",
    "    # new_r_2 = r2(predictions2, test_data2, iden)\n",
    "        \n",
    "    # print(\"The new r^2 is: \", new_r_2, \". This value may be lowe or higher than the original r^2 of the model depending on the importance of the features.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f421a0",
   "metadata": {},
   "source": [
    "**Note on Path to File:** Below I have specified the path to the given CSV containing the data. You do not need to change the path for it to work on this notebook, but if you would like to download the code, you may."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb173a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_file = \"data/data.csv\"\n",
    "\n",
    "# Reads the information contained in the CSV\n",
    "df = pd.read_csv(path_to_file)\n",
    "\n",
    "# Turns the identifiers into dummy variables for the regression\n",
    "df = pd.get_dummies(df,prefix='Identifier ', prefix_sep='=', columns=[\n",
    "    'identifier'])\n",
    "\n",
    "# Setting the list predictor variables called cols\n",
    "cols = list(df.columns)\n",
    "\n",
    "cols.remove('target')\n",
    "cols.remove('date')\n",
    "\n",
    "# Running the regression with the given dataset and predictor list\n",
    "#calculate_model(df, cols)\n",
    "\n",
    "model(df, cols)\n",
    "\n",
    "# signals that the code has run to completion\n",
    "print('\\n Model Complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
