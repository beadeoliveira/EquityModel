{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e12e69f",
   "metadata": {},
   "source": [
    "**DESCRIPTION:** This model will run a regression on all of the data by the identifier(s) specified by the user. The regression is completed using SKLearn which utilizes test and training data to fit a learned model to the data. The model will then be used to complete a factor selection and forward/backward factor selection which can be used to fine-tune the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a63024",
   "metadata": {},
   "source": [
    "**How to Run This Code:** Run each segment of code and markdown in order using the 'Run' button above. Once you have run the last segment the code will execute and results will be outputted. It may take a few moments for the code to output results so please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbfb2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from time import time\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bb996a",
   "metadata": {},
   "source": [
    "**Calculating the Model:** The function below calculates and plots our model given a datatable. Comments included in the code provide a description of what is occuring in each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b0011",
   "metadata": {},
   "source": [
    "**Arranging the Data:** This function will arrange our data based on the given dataset to turn the dates into the index of the graph to allow for the regression to occur over the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f398fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_data(data, iden):\n",
    "    \n",
    "    data.set_index(pd.DatetimeIndex(data['date']), inplace=True)\n",
    "\n",
    "    df = data.loc[data['identifier'] == iden]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfff7a6",
   "metadata": {},
   "source": [
    "**Getting the Training and Testing Data:** This code sets the predictor values, i.e. the independent variables that will be used in the regression and uses the train_test_split to randomly select 30% of the data as testing data, saving the rest for the creation/training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd42fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test_data(data, predictor_list):\n",
    "\n",
    "    predictors = predictor_list\n",
    "\n",
    "    train, test = train_test_split(data, test_size=0.3)\n",
    "    \n",
    "    return [train, test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317b46c",
   "metadata": {},
   "source": [
    "**Running the Linear Regression:** This function conducts the linear regression by fitting thr model to the predictors and training data. It then creats predictions for the data using model.predict and using the index we set previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee55b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(train, test, predictors):\n",
    "    \n",
    "    # Defines the model as a Linear Regression\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Fits the model using the predictors above and defined target training data\n",
    "    model.fit(train[predictors], train[\"target\"])\n",
    "\n",
    "    # Creates the predictors using the model.predict\n",
    "    preds = model.predict(test[predictors])\n",
    "    preds = pd.Series(preds, index=test.index)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203f5f9",
   "metadata": {},
   "source": [
    "**Calculating r^2:** This function calculates the r^2 of the model by utilizing the r2_score function with the test target data and predictions from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d679d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(preds, test, iden):\n",
    "    \n",
    "    # Calculates the r^2 score based on the test values and the predicted values\n",
    "    r = r2_score(test[\"target\"], preds)\n",
    "    \n",
    "    print(\"r2 of \" + iden + \" =\" + str(r))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87356507",
   "metadata": {},
   "source": [
    "**Graphing the Results:** This function creates a graph that plots the predictions of the model against the true values of the test target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44777a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_preds_vs_test(preds, test, iden):\n",
    "    # Creates a graph that plots the predictions of the model against true values\n",
    "    combined = pd.concat({\"target\": test[\"target\"], \"Predictions\": preds},\n",
    "                         axis=1)\n",
    "\n",
    "    # Plots the origional vs predicted\n",
    "    combined.plot()\n",
    "    plt.title(iden)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5f6d58",
   "metadata": {},
   "source": [
    "**K Fold Test:** Conducts a k_fold test on the data using the predictors (X) and response variables (y) using the KFold function. It then Calculates the cross validation score which is later used to calculate the Root Mean Squared Error. The RMSE is useful as it provides information regarding the accuracy of our model - i.e. the lower the RMSE the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3b5b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_test(predictor_list, train, model, X, y):\n",
    "\n",
    "    # Conducts the K_Fold test\n",
    "    cv = KFold(n_splits=10, random_state=100, shuffle=True)\n",
    "\n",
    "    scores = cross_val_score(model, train[predictor_list], train[\"target\"],\n",
    "                             scoring='neg_mean_absolute_error',\n",
    "                             cv=cv, n_jobs=-1)\n",
    "    \n",
    "    print(\"root mean squared error (RMSE) = \" + str(np.sqrt(np.mean(np.absolute(\n",
    "        scores)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b4aeec",
   "metadata": {},
   "source": [
    "**Features:** Returns the features to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f33b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_features():\n",
    "    return ['sector','market_cap', 'index_membership', 'factor_1',\n",
    "                'factor_2', 'factor_3', 'factor_4', 'factor_5', 'factor_6',\n",
    "                'factor_7', 'factor_8', 'factor_9', 'factor_10']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf64d00",
   "metadata": {},
   "source": [
    "**Set X:** Defines our X, i.e. predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_X(data, predictor_list):\n",
    "    return data[predictor_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3e53e",
   "metadata": {},
   "source": [
    "**Set Y:** Defines our y, i.e.  response variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a9501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_y(data):\n",
    "    return data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357588b6",
   "metadata": {},
   "source": [
    "**Ridge Calculation:** Uses RidgeCV to compute efficient Leave-One-Out Cross-Validation. This is an itteration of the Kfold where the number of folds equals the number of instances in the data set. This is advantageous as it is usefull in models that may have multicollinearity.\n",
    "* Used in feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc3487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_calculation(X, y):\n",
    "    \n",
    "    ridge = RidgeCV(alphas = np.logspace(-6, 6, num=5)).fit(X, y)\n",
    "    \n",
    "    return ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec190364",
   "metadata": {},
   "source": [
    "**Calculating the Model:** The function below calculates and plots our model given a datatable. Comments included in the code provide a description of what is occuring in each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38850976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_idents(data, predictor_list, lst):\n",
    "    \n",
    "    # This loops will run the model for each input specified by the user\n",
    "    \n",
    "    for iden in lst:\n",
    "        \n",
    "        print(iden + \": \\n\")\n",
    "    \n",
    "        # sets a new dataframe equal to the original data\n",
    "        # this is done so no information is lost in each itteration as the datatable is modified\n",
    "    \n",
    "        ds = arrange_data(data, iden)\n",
    "\n",
    "        model = LinearRegression()\n",
    "\n",
    "        train_data = get_train_and_test_data(ds, predictor_list)[0]\n",
    "\n",
    "        test_data = get_train_and_test_data(ds, predictor_list)[1]\n",
    "\n",
    "        predictions = linear_model(train_data, test_data, predictor_list)\n",
    "\n",
    "        r_2 = r2(predictions, test_data, iden)\n",
    "\n",
    "        graph_preds_vs_test(predictions, test_data, iden)\n",
    "\n",
    "        X = set_X(ds, predictor_list)\n",
    "\n",
    "        y = set_y(ds)\n",
    "\n",
    "        k_fold_test(predictor_list, train_data, model, X, y)\n",
    "\n",
    "        ridge = ridge_calculation(X, y)\n",
    "\n",
    "        feature_importance(X, y, ridge, iden)\n",
    "\n",
    "        # Selects the most relevent features using forward and backward\n",
    "        # Sequential Feature Selection\n",
    "\n",
    "        features = set_features()\n",
    "\n",
    "        # TO:DO comment out this once you have run the function once\n",
    "\n",
    "        seq_selection(X, y, features, ridge, iden)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99256d",
   "metadata": {},
   "source": [
    "**Feature Importance of Coefficients using RidgeCV:** These functions will utilize the RidgeCV computed in the above function to determine the most important fratures based on a relevency threshhold. It also plots the feature importance graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c56cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple helper function to go from list of True and False to the final selected values\n",
    "\n",
    "def selecting(lst, features):\n",
    "    selected = []\n",
    "    for i in range(0,13):\n",
    "        if lst[i] == True:\n",
    "            selected.append(features[i])\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68242dfb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def feature_importance(X, y, ridge, iden):\n",
    "        \n",
    "    # sets the importance as a list of the ridge coefficients calculated for each feature\n",
    "    importance = list(np.abs(ridge.coef_))\n",
    "\n",
    "    # re-itterates the features, but using better spacing for graph visibility\n",
    "    features = ['sector','market \\n cap', 'index \\n membership', 'factor_1',\n",
    "            'factor_2', 'factor_3', 'factor_4', 'factor_5', 'factor_6',\n",
    "            'factor_7', 'factor_8', 'factor_9', 'factor_10']\n",
    "\n",
    "    # preps a dictionary which scales each 'importance score' by 1000 for visbility\n",
    "    dta = {}\n",
    "    importance_2 = []\n",
    "    \n",
    "    for i in range(0,13):\n",
    "        importance_2.append(importance[i] *1000)\n",
    "        dta[features[i]] = importance_2[i]\n",
    "    \n",
    "    # graphs the factors vs importance level\n",
    "    names = list(dta.keys())\n",
    "    values = list(dta.values())\n",
    "\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(16, 3), sharey=True)\n",
    "    plt.bar(names, values)\n",
    "    fig.suptitle('Feature importances via coefficients of ' + iden + ' (scaled by 1000)')\n",
    "\n",
    "    # Selecting features based on importance\n",
    "    \n",
    "    # sets a threshold parameter and will select the features whose importance (defined by the coefficients) are above this threshold\n",
    "    # This specific threshold will select 3 features by setting the threshold slightly above the coefficient of the fourth most important feature\n",
    "    threshold = np.sort(importance)[-4] + 0.01\n",
    "    # time is used to show the user how long the calculation took - start\n",
    "    tic = time()\n",
    "    # selects the features from the model that are above the threshhold\n",
    "    sfm = SelectFromModel(ridge, threshold=threshold).fit(X, y)\n",
    "    # end time calculation\n",
    "    toc = time()\n",
    "    # returns the list of features found in sfm, i.e. those above the threshhold\n",
    "    lst = list(sfm.get_support())\n",
    "    # prints the results\n",
    "    print(f\"Features of\" + iden +  \" selected by SelectFromModel:\", selecting(lst, features))\n",
    "    print(f\"Done in {toc - tic:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b802c",
   "metadata": {},
   "source": [
    "**Selecting features with Sequential Feature Selection:** **Selecting features with Sequential Feature Selection:** SFS is a greedy procedure where, at each iteration, we choose the best new feature to add to our selected features based a cross-validation score (starts with 0 features and choose the best single feature with the highest score. The procedure is repeated until we reach the desired number of selected features) Our function below does this in forward and backward SFS (the reverse of what is described above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_selection(X, y, features, ridge, iden):\n",
    "\n",
    "    # Forward Selection\n",
    "    \n",
    "    # starts the time\n",
    "    tic_fwd = time()\n",
    "    # runs the SequentialFeatureSelector on our ridge values previosuly calculated\n",
    "    # and specifies that this is to be done in the forward direction\n",
    "    sfs_forward = SequentialFeatureSelector(\n",
    "        ridge, n_features_to_select=2, scoring='r2', direction=\"forward\"\n",
    "    ).fit(X, y)\n",
    "    # stops the time\n",
    "    toc_fwd = time()\n",
    "    \n",
    "    # Backward Selection\n",
    "    \n",
    "    # starts the time\n",
    "    tic_bwd = time()\n",
    "    # runs the SequentialFeatureSelector on our ridge values previosuly calculated\n",
    "    # and specifies that this is to be done in the backward direction\n",
    "    sfs_backward = SequentialFeatureSelector(\n",
    "        ridge, n_features_to_select=2, scoring='r2', direction=\"backward\"\n",
    "    ).fit(X, y)\n",
    "    # stops the time\n",
    "    toc_bwd = time()\n",
    "\n",
    "    #prints the results\n",
    "    print(\n",
    "        \"Features of\" + iden + \" selected by forward sequential selection: \", selecting(sfs_forward.get_support(), features)\n",
    "    )\n",
    "    print(f\"Done in {toc_fwd - tic_fwd:.3f}s\")\n",
    "    print(\n",
    "        \"Features of\" + iden + \" selected by backward sequential selection: \", selecting(sfs_backward.get_support(), features)\n",
    "    )\n",
    "    print(f\"Done in {toc_bwd - tic_bwd:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c71111",
   "metadata": {},
   "source": [
    "**Note on Path to File:** Below I have specified the path to the given CSV containing the data. You do not need to change the path for it to work on this notebook, but if you would like to download the code, you may."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e776c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_to_file = \"data/data.csv\"\n",
    "\n",
    "selected = ''\n",
    "\n",
    "# Reads the information contained in the CSV\n",
    "og_df = pd.read_csv(path_to_file)\n",
    "\n",
    "# Prepares a list to hold the identifiers\n",
    "idents = []\n",
    "\n",
    "identifier_set = list(set(og_df[\"identifier\"].values.tolist()))\n",
    "\n",
    "# While loop that will continue to ask the user for input until they answer \n",
    "# \"N\". Each identifier that is added is checked to determine if it is valid.\n",
    "\n",
    "\n",
    "answer = 'Y'\n",
    "while (answer == 'Y'):\n",
    "    n = input(\"What identifier would you like to model? (i.e. KM943MN5D7E3) \\n\")\n",
    "    if(n in identifier_set):\n",
    "        idents.append(n)\n",
    "        answer = input(\n",
    "        \"Would you like to add another? - answer 'Y' for yes and \"\n",
    "        \"'N' for no \\n\")\n",
    "    else:\n",
    "        answer = input(\"Your input is not an identifier. Would you like to try again? - answer 'Y' for yes and \"\n",
    "        \"'N' for no \\n\")\n",
    "        \n",
    "predictors = ['market_cap', 'sector', 'index_membership', 'factor_1',\n",
    "                  'factor_2', 'factor_3', 'factor_4', 'factor_5', 'factor_6',\n",
    "                  'factor_7', 'factor_8', 'factor_9', 'factor_10']\n",
    "        \n",
    "\n",
    "# Runs the model on the given identifiers\n",
    "individual_idents(og_df, predictors, idents)\n",
    "\n",
    "# signals that the code has run to completion\n",
    "print('\\n Model Complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
